# we can thank Bhavya for this
# import the goods
import sqlite3
from functools import reduce
import os
import re

# Function to map specific types to generic types
def map_generic_type(data_type):
    # text types
    if re.match(r"TEXT(\(\d+\))?", data_type, re.IGNORECASE):
        return "Str"
    # integer types
    elif re.match(r"(INTEGER|INT|MEDIUMINT|SMALLINT|TINYINT)", data_type, re.IGNORECASE):
        return "Int"
    # real, double, and other float
    elif re.match(r"(REAL|DOUBLE|FLOAT|NUMERIC|DECIMAL)", data_type, re.IGNORECASE):
        return "Float"
    # date/time types
    elif re.match(r"(DATE|DATETIME|TIME|TIMESTAMP)", data_type, re.IGNORECASE):
        return "Date"
    # other types same as defined
    else:
        return data_type.upper()

def check_column_consistency(gpkg_path):

    gpkg_dir = os.path.dirname(gpkg_path)
    gpkg_name = os.path.splitext(os.path.basename(gpkg_path))[0]
    log_file_path = os.path.join(gpkg_dir, f"{gpkg_name}_log.txt")

    # Connect to the GeoPackage
    conn = sqlite3.connect(gpkg_path)
    cursor = conn.cursor()

    # Query for table names, excluding autogenerated tables, rtree tables, and sqlite_sequence
    query = """
    SELECT name FROM sqlite_master 
    WHERE type='table' 
    AND name NOT LIKE 'gpkg_%'  -- Exclude metadata tables
    AND name NOT LIKE 'rtree_%' -- Exclude R-tree tables
    AND name != 'sqlite_sequence'; -- Exclude sqlite_sequence table
    """
    table_names = cursor.execute(query).fetchall()

    # Dictionary to hold column names and types for each layer
    layer_columns = {}

    # Fetch column names and types using PRAGMA table_info and map to generic types
    for (table_name,) in table_names:
        cursor.execute(f"PRAGMA table_info({table_name});")
        columns = cursor.fetchall()

        # Store the column name and generic type for each layer
        layer_columns[table_name] = {col[1]: map_generic_type(col[2]) for col in columns}  # col[1] is name, col[2] is type

    # Find columns common to all layers
    common_columns = reduce(lambda x, y: x & y.keys(), layer_columns.values())
    common_columns_with_types = {col: layer_columns[next(iter(layer_columns))][col] for col in common_columns}

    # Identify unique columns per layer
    unique_columns_per_layer = {
        table: {col: col_type for col, col_type in columns.items() if col not in common_columns}
        for table, columns in layer_columns.items()
    }

    # Write results to the log file in the same directory as the GeoPackage
    with open(log_file_path, "w") as log_file:
        log_file.write("Columns common to all layers (name: type):\n")
        for col_name, col_type in common_columns_with_types.items():
            log_file.write(f"  {col_name}: {col_type}\n")

        log_file.write("\nUnique columns in each layer (name: type):\n")
        for table, unique_columns in unique_columns_per_layer.items():
            log_file.write(f"\nLayer '{table}':\n")
            if unique_columns:
                for col_name, col_type in unique_columns.items():
                    log_file.write(f"  {col_name}: {col_type}\n")
            else:
                log_file.write("  No unique columns\n")

    # Close the database connection
    conn.close()

    print(f"Results logged to {log_file_path}")

# Prompt the user to input the GeoPackage path
gpkg_path = input("Please enter the path to the GeoPackage file: ")
check_column_consistency(gpkg_path)

# gpkg_path = 'C:/Users/sreddyps/Downloads/test_miltilayer_crime_different_col_names_same_data.gpkg'
# gpkg_path = 'C:/Users/sreddyps/Downloads/test_multilayer_crime_same_col_names_diff_data_types.gpkg'
# gpkg_path = 'C:/Users/sreddyps/Downloads/test_multilayer_crime_same_col_names_missing_data_some_rows.gpkg'
# check_column_consistency(gpkg_path)
